---
title: LeNet CNN卷积神经网络学习
category: dnn
tags: [cnn,神经网络,卷积,LeNet]
author: weiwei
---

* TOC
{:toc}


# 什么是卷积神经网络？
卷积神经网络是一种特定网络拓扑的神经网络，在图像、语音领域证明了其高精度高效率的能力，在语言处理领域也逐渐取得了很多突破，比如最近[Facebook提出的cnn机器翻译](https://36kr.com/p/5074276.html)号称超越了谷歌。本文学习了LeNet网络架构，并基于此写的一篇学习笔记。

# LeNet网络架构
下图是一个LeNet网络结构，输出的分类标签是0-9。整个网络包括两个Convolution+Pooling层，以及后面的全连接网络。

![](http://ocs628urt.bkt.clouddn.com/LeNet_1.jpg)

对于一个手写输入8的执行的执行流程如下：

![](http://ocs628urt.bkt.clouddn.com/conv_all.png)

## Convolution(卷积)
卷积是图像领域的概念，主要用于从图像中抽取特征，比如边缘、锐化、模糊等。下图是一个卷积操作的动态示意：

![](http://7d9q8z.com1.z0.glb.clouddn.com/convolution_schematic.gif)

这里面的输入是:

![](http://7d9q8z.com1.z0.glb.clouddn.com/screen-shot-2016-07-24-at-11-25-13-pm.png)

卷积核(也叫滤波器)是:

![](http://7d9q8z.com1.z0.glb.clouddn.com/screen-shot-2016-07-24-at-11-25-24-pm.png)

在图像处理中有各种不同的卷积核:

![](http://7d9q8z.com1.z0.glb.clouddn.com/screen-shot-2016-08-05-at-11-03-00-pm.png)

在LeNet中我们会用不同的卷积来抽取特征，在上面的LeNet架构中我们能看到Convolution输出是一个多层的图像，这就是用不同的核做了上面的卷积操作，下图会更直观一些：

![](http://7d9q8z.com1.z0.glb.clouddn.com/giphy.gif)

这个示意图中有红色的卷积核，也有绿色的卷积核，两次卷积操作得到了两幅不同的图像，这两幅图像在CNN中叫Feature Map。

卷积的结果往往还需要做一次激活函数变换，比如`ReLU`、`sigmoid`、`tanh`等，目前用的比较多的是ReLU，这个就是用到卷积后的结果上，一个一个元素apply就可以了。

注意卷积操作都是有bias的，拿`5x5`的卷积核为例，做一个局部卷积操作的计算量是`5x5+1`这里面`+1`就是bias的计算。

## Pooling(池化)
池化是为了降低特征图的维度，但能保留住核心的信息，常有最大化、平均化、加和等几种，下图是一个max pooling的示意：

![](http://7d9q8z.com1.z0.glb.clouddn.com/screen-shot-2016-08-10-at-3-38-39-am.png)


池化是运用到每一个Feature Map的，每个Feature Map进行Pooling操作后得到一个单独的Pooling后的Feature Map

![](http://7d9q8z.com1.z0.glb.clouddn.com/screen-shot-2016-08-07-at-6-19-37-pm.png)

## 前后两层卷基层的衔接
参看开头的LeNet网络结构，输入是`32x32`,经过Convolution Layer 1(C1)6个卷积核`5x5`,输出6个`28x28`的feature map, 经过S2的pooling(`2x2`)操作，得到`14x14`的pooled feature map；LeNet Convolution Layer 2(C3)也是卷积层，卷积核大小为`5x5`，卷积核数量为16, 输入是S2的输出(`14x14`)，输出会是16个`10x10`的feature map，但输入是6个feature map，和C1面临的情况不一样，下图解释了下C3是如何处理6个feature map输入得到16个feature map输出：

![](http://ocs628urt.bkt.clouddn.com/lenet-c3table.png)

C3的第一个(横坐标0)卷积核处理前三个(纵坐标0,1,2)输入的feature map, 这里可以分解为分别和这三个输入计算得到3个feature map，这3个feature map再element-wise相加得到一个feature map，以此类推，最终得到16个`10x10`的pooled feature map。

再经过一次Pooling，还是得到16个pooled feature map，每个pooled feature map的大小是`5x5`。

## 全连接层
这一层也是卷积操作，但卷积核是120个，大小是`5x5`，上一层输出的16个pooled feature map的大小是`5x5`，这个卷积后就会得到120个`1x1`的feature map。

这一层的处理方式是对于每一个卷积核，和16个输入分别计算卷积，得到16个1x1，这所有的结果sum得到一个`1x1`的输出，由于有120个卷积和，所以会得到120个`1x1`的输出，进而作为全连接部分的输入。

再下一层是84个神经元，输出层是10个神经元。


# 参考
* [LeNet神经网络](http://noahsnail.com/2017/03/02/2017-3-2-LeNet%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)
* [神经网络的白话版](http://blog.csdn.net/qq_34420188/article/details/71082570)
